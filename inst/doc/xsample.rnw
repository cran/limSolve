\documentclass[a4paper]{article} %
%\usepackage{a4wide}
\usepackage{graphicx}
\usepackage{amsmath}
%\usepackage{lscape}
%\usepackage{wasysym}
%\usepackage{wrapfig}
%\usepackage{here}
%\usepackage{natbib}
\usepackage{amssymb}
\usepackage{mathptmx}
\usepackage[latin1]{inputenc}      % á,é,ë,...

%\linespread{2}
\usepackage{Sweave}
%\VignetteIndexEntry{limSolve}

\begin{document}

% \begin{frontmatter}

\title{xsample: an R function for sampling over- and underdetermined linear inverse problems}

\author{Karel Van den Meersche}

\maketitle

\section{Introduction}

xsample is an R function that instead of optimizing a linear problem,
returns a sample set that approaches the probability distribution of
the solution. 

A general linear problem can be written in matrix notation as:
\footnote{notations: vectors and matrices are in \textbf{bold};
  scalars in normal font. Vectors are indicated with a small letter;
  matrices with capital letter. Indices between brackets indicate
  elements of vectors (as in
  $a_{(i)}$) or matrices (as in
  $A_{(i,j)}$). Rows or columns of matrices are indicated as $\mathbf{A}_{(i,)}$
  (rows) or $\mathbf{A}_{(,j)}$ (columns). Indices without brackets ($\mathbf{q_1}$,
  $\mathbf{q_2}$) indicate vectors that are subsequent in a random walk. }
\begin{equation}
  \label{eq:1}
    \begin{cases}		
      \mathbf{A}\mathbf{x}\simeq\mathbf{b} \\
      \mathbf{E}\mathbf{x}=\mathbf{f} \\
      \mathbf{G}\mathbf{x}\geq\mathbf{h}
    \end{cases}
\end{equation}

A typical linear model consists of a set of equality constraints that have to be met 
either exactly or approximately, and a number of inequality constraints. When there are as 
many independent and consistent equations as there are unknowns, there is one set of parameters that meets all the 
equations, and the model can be called evendetermined. In other cases, there are more equations 
than unknowns; there is no exact solution and the model is overdetermined. An approximate 
solution can then be calculated by minimizing some model cost, such as the sum of squared 
residuals. Linear regressions are examples of such models. 

When there are too few equality constraints, the model is underdetermined, and there exist
an infinite number of parameter sets that meet the constraints. In many research areas such as 
engineering, economics, one is interested in only one optimal solution. Thus, one looks for 
the simplest solution, or the solution that minimizes cost, or maximizes security, profit, efficiency or 
other variables. This kind of problems are solved with linear programming and quadratic programming techniques. 

Instead of searching one optimal solution, xsample() samples the whole
solution space, using a Markov Chain Monte Carlo (MCMC)
algorithm.


\section{Method}

\subsection{Step 1: eliminate equality constraints} %transform x to q
The elements $x_{(i)}$ of the solution vector \textbf{x} are not linearly independent; they
are coupled through the equations in $\mathbf{Ax}=\mathbf{b}$. They
are first linearly transformed to a vector \textbf{q} for which all elements
$q_{(i)}$ are linearly independent. If a vector \textbf{p} is a particular
solution of equation (\ref{eq:1}), then all solutions \textbf{x} can be written as:
\begin{equation}
  \label{eq:3}
  \mathbf{x}= \mathbf{p} + \mathbf{Z}\mathbf{q}
\end{equation}
\textbf{Z} is an orthonormal matrix obtained e.g. from the QR-decomposition of \textbf{A} and
a basis for the null space of \textbf{A}: $\mathbf{A}\mathbf{Z}=\mathbf{0}$ and 
$\mathbf{Z}^T\mathbf{Z}=\mathbf{I}$. \\
A particular solution \textbf{p} can be provided if found easily. If
not, a particular solution is calculated using LSEI
\cite{Haskell1981}.

There are no equality constraints for the elements in \textbf{q}. The
inequality constraints can be rewritten as: 
\begin{subequations}
\begin{equation}
  \label{eq:6}
  \mathbf{G}(\mathbf{p}+\mathbf{Zq})\geq \mathbf{h}
\end{equation}
\begin{equation}
	\label{eq:7}
	\mathbf{GZq} + (\mathbf{Gp}-\mathbf{h}) \geq \mathbf{0}
\end{equation}
\end{subequations}
The problem of finding \textbf{x} with equality and inequality
constraints, has been translated into the problem of sampling
\textbf{q} with only inequality
constraints. \\
Because \textbf{p} meets the inequality constraints $\mathbf{Gp}\geq
\mathbf{h}$, there is already one trivial solution of \textbf{q}: the
null vector \textbf{0}. From this point, it is possible to sample new
points.

The probability of \textbf{q} is a product of the probability of
\textbf{x} and the Jacobian determinant:
\begin{equation}
  \label{eq:10}
  p(\mathbf{q}) = p(\mathbf{x}) ||\frac{\partial \mathbf{x}}{\partial \mathbf{q}}||
\end{equation}
In this case, the Jacobian is $|\mathbf{Z}|=1$. Therefore $p(\mathbf{x})=p(\mathbf{q})$.


\subsection{Step 2: random walk}

\subsubsection{Markov Chain}
The probability distribution of \textbf{q} can be sampled numerically
using a random walk. A Metropolis algorithm \cite{Roberts1996}
produces a series of samples of the solution space of which the
distribution approximates the true probability distribution. New
samples $\mathbf{q_2}$ are drawn randomly from a jump distribution
$j(.|\mathbf{q_1})$ that only depends on the previously accepted point
$\mathbf{q_1}$. The new sample point is either accepted or rejected based on
the following criterion:
\begin{equation}
  \label{eq:9}
  \mbox{if} \quad  r \le \frac{p(\mathbf{q_2})}{p(\mathbf{q_1})} \quad \mbox{accept $\mathbf{q_2}$
  else accept $\mathbf{q_1}$}
\end{equation}
$r$ is sampled randomly between 0 and 1. The only prerequisite for the
sample distribution to converge to the true probability distribution,
is that the jump distribution from which a new sample is drawn, is
symmetrical: the probability to jump from $\mathbf{q_1}$ to
$\mathbf{q_2}$, $j(\mathbf{q_2}|\mathbf{q_1})$, has to be the same as
the probability to jump from $\mathbf{q_2}$ to $\mathbf{q_1}$,
$j(\mathbf{q_1}|\mathbf{q_2})$. 

Three jump algorithms for selecting new samples were implemented: Two
hit-and-run algorithms \cite{Smith1984} and one algorithm
that uses the inequality bounds as reflective planes. . All three
algorithms fulfill the symmetry prerequisite for the metropolis
algorithm.


\subsubsection{Random Directions Algorithm (rda) \cite{Smith1984}}
The algorithm exists of two steps: first it selects a random
direction. This direction together with the starting point define a
line in solution space. In step 2, it searches the intersections of this
line with the planes defined by the inequality constraints. A new
point is then sampled uniformly along the line segment that fulfills all
inequalities. 

\subsubsection{Coordinates Directions Algorithm (cda) \cite{Smith1984}}
Very similar to the random directions algorithm, this algorithm starts
with selecting a direction along one of the coordinate axes. The rest
of the algorithm is analogous to the random directions algorithm. This
proves to be very efficient for linear problems.

\paragraph{NOTE:} 
rda and cda only work if G and H define a convex solution space. In an
open or half open space, these algorithms will spawn error
messages. 

\subsubsection{mirror}

This algorithm was inspired by the reflections in mirrors and uses the
inequality constraints as reflecting planes. New samples are taken
from a normal distribution with $\mathbf{q_1}$ as average and a fixed
standard deviation, called the jump length. This jump length has a
significant influence on the efficiency of the algorithm .

\paragraph{random walk with inequality constraints}
When jumping with a Gaussian jump distribution in a highdimensional space 
with inequality constraints, the chance to jump out of the accepted
range is exponential to the number of inequality constraints. To make
sure every new sample fulfills all inequalities, the inequalities are
used as mirrors. 

In a euclidean space, every inequality constraint defines a boundary
of the feasible subspace. Each boundary can be considered a
multidimensional plane (a hyperplane). One side of the hyperplane is the feasible range,
where the inequality is fulfilled. The other side of the hyperplane is
non-feasible. The hyperplanes are determined by the following set of equations: 
\begin{equation}
  \label{eq:11}
  (\mathbf{GZ})_{(,i)}\mathbf{q} + (\mathbf{Gp})_{(i)} - h_{(i)}=0 \; \forall i
\end{equation}
If $\mathbf{q_1}$ is a point for which the
equality constraints are fulfilled, a new point $\mathbf{q_2}$ can be sampled in the following way: 
first $\mathbf{q_{2-0}}$ is sampled in the unrestricted space, ignoring all
inequality constraints. 
\begin{equation}
  \label{eq:4}
  \mathbf{q_{2-0}}=\mathbf{q_1}+\mathbf{\epsilon}
\end{equation}
If $\mathbf{q_{2-0}}$ is in the feasible range (all equalities are met), $\mathbf{q_{2-0}}$ is
accepted as a sample point and becomes the new starting point $\mathbf{q_1}$
for further sampling.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.75\textwidth]{Mirrortechnique-fig01}
  \caption{MCMC jump with inequality constraints functioning as mirrors. See text for explanation.}
  \label{Fig:1}
\end{figure}

If there are unmet inequalities (figure \ref{Fig:1}), then the new point $\mathbf{q_{2-0}}$
is mirrored consecutively in the hyperplanes representing the unmet inequalities: the line segment $\mathbf{q_1}
\rightarrow \mathbf{q_{2-0}}$ crosses these hyperplanes. For each hyperplane, a scalar $\alpha_{(i)}$ can
be calculated for which
\begin{equation}
  \label{eq:8}
  (\mathbf{GZ})_{(,i)}( \mathbf{q_1}+\alpha_{(i)}\mathbf{\epsilon}) + (\mathbf{Gp})_{(i)} - h_{(i)}=0
\end{equation}
with $\mathbf{\epsilon}=\mathbf{q_{2-0}}-\mathbf{q_1}$. The hyperplane with the smallest non-negative $\alpha_{(i)}$, call it
$\alpha_{(s)}$, is the hyperplane that is crossed first by the line segment. 
$\mathbf{q_{2-0}}$ is mirrored around this hyperplane.
If the new point ($\mathbf{q_{2-1}}$ in figure \ref{Fig:1}) still has
unmet inequalities, a new set of $\alpha_{(i)}$'s is calculated from the line segment 
between the new point and the intersection of the previous line segment and 
the first hyperplane: $\mathbf{q_1}+\alpha_{(s)}\mathbf{\epsilon}$. \\
$\mathbf{q_{2-1}}$ is again reflected
in the hyperplane with smallest non-negative $\alpha_{(i)}$. This is repeated until all
inequalities are met. The resulting point $\mathbf{q_2}$ is in the feasible subspace and
is accepted as a new sample point. 


\begin{thebibliography}{}

\bibitem{Haskell1981} Haskell, K.H., Hanson, R.J.: An algorithm for
  linear least-squares problems with equality and non-negativity
  constraints. Mathematical Programming \textbf{21}(1), 98-118 (1981).
\bibitem{Roberts1996} Roberts, G.: Markov chain concepts related to
  sampling algorithms. In: W. Gilks, S. Richardson, D. Spiegelhalter
  (eds.) Markov Chain Monte Carlo in practice, pp. 45-58.  Chapman \&
  Hall (1996)
\bibitem{Smith1984} Smith, R.L.: Efficient Monte Carlo Procedures for
  Generating Points Uniformly Distributed over Bounded Regions.
  Operations Research \textbf{32}, pp. 1296-1308 (1984).

\end{thebibliography}

%% \clearpage
%% \section*{Appendix: Implementation of the algorithm in R}

%% \begin{verbatim}
%% library(Inverse)
%% library(MASS)

%% xsample <- function(E=NULL,              #Ex=F
%%                     F=NULL,    
%%                     G=NULL,             #Gx>H; 
%%                     H=NULL,             #Gx>H; 
%%                     A=NULL,             #Ax~=B
%%                     B=NULL,
%%                     sdB=1,              #standard deviations on B (weighting)
%%                     probability=dnorm,  #Ax~=B; use normal distribution for approximation
%%                     iter=3000,          #number of iterations
%%                     algorithm=mirror,   # one of mirror, cda, da ; cda and da need to have a closed space (inequality constraints)!!
%%                     jmp=.1,             #jump length of the transformed variables q: x=p+Zq (only if algorithm=mirror)
%%                     accuracy=sqrt(.Machine$double.eps), # accuracy of Z,g,h: smaller numbers are set to zero to avoid rounding errors
%%                     x0=NULL)             #particular solution 
%% {
%% 
%%   ## find a particular solution x0
%%   if (is.null(x0))
%%     {
%%       l <- lsei(A=A,B=B,E=E,F=F,G=G,H=H)
%%       if (l$residualNorm>1e-6)
%%         stop("no particular solution found;incompatible constraints")
%%       else
%%         x0 <- l$X
%%     }
%%   n <- length(x0)   
%%   
%%   ## Z is an orthogonal matrix for which E%*%Z=0; it can serve as basis for the null space of E.
%%   ## all solutions for the equalities have the form x = x0 + Zq with q a random vector. 
%%   ## the vector q is varied in a random walk, using a MCMC with acceptance rate = 1. The inequality constraints Gx>H
%%   ## can be rewritten as Gx0-H + (GZ)q >0
%%   
%%   if (!is.null(E))
%%     {
%%       Z <- Null(t(E)); Z[abs(Z)<accuracy] <- 0  #x=x0+Zq ; EZ=0
%%     } else { Z <- diag(n) }
%%   
%%   if (!is.null(G))
%%     {
%%       g <- G%*%Z
%%       h <- H-G%*%x0                                            #gq-h>=0
%%       g[abs(g)<accuracy] <- 0
%%       h[abs(h)<accuracy] <- 0
%%     } else { g <- G; h <- H }
%%   
%% 
%%   if(!is.null(A))
%%     {
%%       a <- A%*%Z
%%       b <- B-A%*%x0                          #aq-b~=0
%%       prob <- function(q) prod(probability(b,a%*%q,sdB))
%%       test <- function(q2) (prob(q2)/prob(q1))>runif(1) #metropolis criterion
%%     } else {
%%       prob <- function(q) 1
%%       test <- function(q2) TRUE
%%     }
%% 
%%   k <- ncol(Z)
%%   q1 <- rep(0,k)
%%   x <- matrix(ncol=n,nrow=iter,dimnames=list(NULL,colnames(A)))
%%   x[1,] <- x0
%% 
%%   p <- vector(length=iter) # probability distribution
%%   p[1] <- prob(q1)
%%   naccepted <- 1
%%   
%%   for (i in 2:iter)
%%     {
%%       q2 <- algorithm(q1,g,h,k,jmp)
%%       if (test(q2)) { q1 <- q2 ; naccepted=naccepted+1}
%%       x[i,] <- x0+Z%*%q1
%%       p[i] <- prob(q1)
%%     }
%% 
%%   return(list(x=x,                      # matrix with samples for x
%%               p=p,                      # probability vector for all samples
%%               acceptedratio=naccepted/iter)) # ratio of acceptance
%% }
%% 
%% mirror <- function(q1,g,h,k=length(q),jmp)
%%   ## function ensuring that a jump from q1 to q2
%%   ## fulfills all inequality constraints formulated in g and h
%%   ## gq=h can be seen as equations for planes that are considered mirrors. when a jump crosses one or more of these
%%   ## mirrors, the vector describing the jump is deviated according to rules of mirroring.
%%   ## the resulting new vector q will always be within the subspace of R^n for which all inequalities are met.
%%   ## also are the requirements for a MCMC met: the probability in the subspace is constant,
%%   ## the probability out of the subspace is 0.
%%   ## q1 has to fulfill constraints by default!
%%   ## Karel Van den Meersche 20070921
%%   {
%%     ##if (any((g%*%q1)<h)) stop("starting point of mirroring is not in feasible space")
%%     q2 <- rnorm(k,q1,jmp)
%%     if (!is.null(g))
%%       {
%%         x2 <- g%*%q2-h
%%         q10 <- q1
%%         
%%         while (any(x2<0))                 #mirror
%%           {
%%             epsilon <- q2-q10                       #vector from q1 to q2: our considered light-ray that will be mirrored at the boundaries of the space
%%             w <- which(x2<0)                        #which mirrors are hit?
%%             alfa <- ((h-g%*%q10)/g%*%epsilon)[w]    #alfa: at which point does the light-ray hit the mirrors? g*(q1+alfa*epsilon)-h=0
%%             whichminalfa <- which.min(alfa)
%%             j <- w[whichminalfa]                    #which smallest element of alfa: which mirror is hit first?
%%             d <- -x2[j]/sum(g[j,]^2)     #add to q2 a vector d*Z[j,] which is oriented perpendicular to the plane Z[j,]%*%x+p; the result is in the plane. 
%%             q2 <- q2+2*d*g[j,]                      #mirrored point
%%             x2 <- g%*%q2-h
%%             q10 <- q10+alfa[whichminalfa]*epsilon   #point of reflection
%%           }
%%       }
%%     q2
%%   }
%% 
%% ## hit-and-run algorithms
%% ## modeled after Smith, R.L. Efficient Monte Carlo Procedures for
%% ## Generating Points Uniformly Distributed over Bounded Regions. Operations Research 32, pp. 1296-1308,1984.
%% 
%% cda <- function(q,g,h,k=length(q),...)            # coordinates direction algorithm
%%                                         # samples a new point in the feasible range of the solution space along a random coordinate
%%   {
%%     i <- sample(1:k,1)                  #
%%     h1 <- h-g[,-i]%*%q[-i]              # g[,i]q[i]>h1
%%     maxqi <- min((h1/g[,i])[g[,i]<0])
%%     minqi <- max((h1/g[,i])[g[,i]>0])
%%     q[i] <- runif(1,minqi,maxqi)
%%     return(q)
%%   }
%% 
%% rda <- function(q,g,h,k=length(q),...)            #random direction algorithm
%%                                         #samples a new point in the feasible range of the solution space in a random direction
%% {
%%   ##if (any((g%*%q)<h)) stop("starting point is not in feasible space")
%%   e <- rnorm(k)
%%   d <- e/norm(e)                        #d: random direction vector; q2 = q + alfa*d
%%   
%%   alfa <- ((h-g%*%q)/g%*%d)             #
%%   alfa.u <- min(alfa[alfa>0])
%%   alfa.l <- max(alfa[alfa<0])
%%   q.u <- q+alfa.u*d
%%   q.l <- q+alfa.l*d
%%   if (any(g%*%q.u<h)) q.u <- q
%%   if (any(g%*%q.l<h)) q.l <- q
%%   minq <- pmin(q.u,q.l)
%%   maxq <- pmax(q.u,q.l)
%%   runif(k,minq,maxq)
%% }
%% 
%% norm <- function(x) sqrt(x%*%x)
%% \end{verbatim}

\end{document}

